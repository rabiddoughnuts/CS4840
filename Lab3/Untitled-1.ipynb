{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including TensorFlow, Keras, NumPy, Matplotlib, and Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.datasets import cifar10, reuters\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Plot the CIFAR10 Data\n",
    "Load the CIFAR10 dataset, convert images to grayscale, preprocess the data, and plot some example images with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the CIFAR10 data\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert images to grayscale\n",
    "X_train_gray = tf.image.rgb_to_grayscale(X_train)\n",
    "X_test_gray = tf.image.rgb_to_grayscale(X_test)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = X_train_gray.numpy()\n",
    "X_test = X_test_gray.numpy()\n",
    "\n",
    "# Data preprocessing\n",
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "y_train, y_test = y_train.astype(np.uint8).squeeze(), y_test.astype(np.uint8).squeeze()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Class names for CIFAR-10\n",
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "               \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# Plot the data\n",
    "def plot_digits(instances, labels, images_per_row=5):\n",
    "    for i in range(len(instances)):\n",
    "        idx = i // images_per_row\n",
    "        idy = i % images_per_row \n",
    "        ax[idx, idy].imshow(instances[i].squeeze(), cmap=\"gray\")\n",
    "        ax[idx, idy].set_title(class_names[labels[i]])\n",
    "        ax[idx, idy].axis(\"off\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 4))\n",
    "example_images = X_train[:10]\n",
    "example_labels = y_train[:10]\n",
    "plot_digits(example_images, example_labels, images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Multi-Layer Perceptron\n",
    "Define and train a multi-layer perceptron using Keras with different dropout rates (0.0, 0.3, 0.5), and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Run your function for each drop rate and print the results\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m drop_rate \u001b[38;5;129;01min\u001b[39;00m drop_rates:\n\u001b[0;32m---> 39\u001b[0m     accuracy, microf1, macrof1 \u001b[38;5;241m=\u001b[39m \u001b[43manswer_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Micro F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmicrof1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Macro F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacrof1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36manswer_one\u001b[0;34m(drop_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer_one\u001b[39m(drop_rate):\n\u001b[0;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      7\u001b[0m         keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m]),  \u001b[38;5;66;03m# Flatten layer with input shape [32, 32, 1]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m300\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Dense hidden layer, 300 neurons, ReLU\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(drop_rate),  \u001b[38;5;66;03m# Dropout layer using drop_rate to address overfitting\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Dense hidden layer, 100 neurons, ReLU\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(drop_rate),  \u001b[38;5;66;03m# Dropout layer using drop_rate to address overfitting\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Dense output layer, 10 neurons, softmax\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     ])\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     16\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Optimization algorithm: adam\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Evaluation metrics: accuracy\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     22\u001b[0m         X_train, y_train,  \u001b[38;5;66;03m# Use X_train, y_train for training\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,  \u001b[38;5;66;03m# epochs, 20\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,  \u001b[38;5;66;03m# batch size, 32\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test)  \u001b[38;5;66;03m# Use X_test, y_test for validation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# Train a Multi-Layer Perceptron\n",
    "\n",
    "drop_rate = 0.0  # Please use 0.0, 0.3, and 0.5, respectively\n",
    "\n",
    "def answer_one(drop_rate):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[32, 32, 1]),  # Flatten layer with input shape [32, 32, 1]\n",
    "        keras.layers.Dense(300, activation=\"relu\"),  # Dense hidden layer, 300 neurons, ReLU\n",
    "        keras.layers.Dropout(drop_rate),  # Dropout layer using drop_rate to address overfitting\n",
    "        keras.layers.Dense(100, activation=\"relu\"),  # Dense hidden layer, 100 neurons, ReLU\n",
    "        keras.layers.Dropout(drop_rate),  # Dropout layer using drop_rate to address overfitting\n",
    "        keras.layers.Dense(10, activation=\"softmax\")  # Dense output layer, 10 neurons, softmax\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",  # Loss function\n",
    "        optimizer=\"adam\",  # Optimization algorithm: adam\n",
    "        metrics=[\"accuracy\"]  # Evaluation metrics: accuracy\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,  # Use X_train, y_train for training\n",
    "        epochs=20,  # epochs, 20\n",
    "        batch_size=32,  # batch size, 32\n",
    "        validation_data=(X_test, y_test)  # Use X_test, y_test for validation\n",
    "    )\n",
    "    \n",
    "    y_proba = model.predict(X_test)  # Calculate prediction probabilities on X_test\n",
    "    y_pred = np.argmax(y_proba, axis=1)  # Obtain the predicted classes from y_proba\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    microf1 = f1_score(y_test, y_pred, average='micro')  # Calculate micro f1\n",
    "    macrof1 = f1_score(y_test, y_pred, average='macro')  # Calculate macro f1\n",
    "    \n",
    "    return accuracy, microf1, macrof1\n",
    "\n",
    "# Run your function in the cell to return the results\n",
    "accuracy_1, microf1_1, macrof1_1 = answer_one(drop_rate)\n",
    "print(accuracy_1, microf1_1, macrof1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Convolutional Neural Network\n",
    "Define and train a convolutional neural network using Keras, and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Convolutional Neural Network\n",
    "\n",
    "def answer_two():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)),  # Convolution layer, input shape (32, 32, 1), 32 channels, kernel (3, 3), ReLU\n",
    "        keras.layers.MaxPooling2D((2, 2)),  # MaxPooling layer, pooling size (2, 2)\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),  # Convolution layer, 64 channels, kernel size (3, 3), ReLU\n",
    "        keras.layers.MaxPooling2D((2, 2)),  # MaxPooling layer, pooling size (2, 2)\n",
    "        keras.layers.Flatten(),  # Flatten layer\n",
    "        keras.layers.Dense(10, activation='softmax')  # Dense output layer, 10 neurons, softmax\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",  # Loss function\n",
    "        optimizer=\"adam\",  # Optimization algorithm: adam\n",
    "        metrics=[\"accuracy\"]  # Evaluation metrics: accuracy\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,  # Use X_train, y_train for training\n",
    "        epochs=10,  # epochs, 10\n",
    "        batch_size=32,  # batch size, 32\n",
    "        validation_data=(X_test, y_test)  # Use X_test, y_test for validation\n",
    "    )\n",
    "    \n",
    "    y_proba = model.predict(X_test)  # Calculate prediction probabilities on X_test\n",
    "    y_pred = np.argmax(y_proba, axis=1)  # Obtain the predicted classes from y_proba\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    microf1 = f1_score(y_test, y_pred, average='micro')  # Calculate micro f1\n",
    "    macrof1 = f1_score(y_test, y_pred, average='macro')  # Calculate macro f1\n",
    "    \n",
    "    return accuracy, microf1, macrof1\n",
    "\n",
    "# Run your function in the cell to return the results\n",
    "accuracy_2, microf1_2, macrof1_2 = answer_two()\n",
    "print(accuracy_2, microf1_2, macrof1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Print the Reuters Data\n",
    "Load the Reuters dataset, preprocess the data, and print some example data with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print the `Reuters` data\n",
    "\n",
    "# Download data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=10000)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Class names for Reuters\n",
    "class_names = [\n",
    "    \"cocoa\", \"grain\", \"crude\", \"money-fx\", \"earn\", \"acq\", \"wheat\", \"corn\",\n",
    "    \"dlr\", \"sugar\", \"coffee\", \"gold\", \"veg-oil\", \"jobs\", \"money-supply\",\n",
    "    \"oilseed\", \"earnings\", \"nat-gas\", \"reserves\", \"cotton\", \"housing\", \"gas\",\n",
    "    \"silver\", \"zinc\", \"tin\", \"income\", \"alum\", \"lead\", \"copper\", \"retail\",\n",
    "    \"carcass\", \"potato\", \"livestock\", \"iron-steel\", \"rubber\", \"citrus\",\n",
    "    \"instal-debt\", \"money-market\", \"heatwave\", \"nickel\", \"fuel-oil\", \"sunseed\"\n",
    "]\n",
    "\n",
    "# Setup hyperparameters\n",
    "vocab_size = 10000  # Number of unique words to consider\n",
    "max_len = 50        # Maximum length of each review (padding/truncation)\n",
    "\n",
    "# Filter out-of-vocabulary indices\n",
    "def filter_out_of_vocab_indices(data, vocab_size):\n",
    "    return [[min(word, vocab_size - 1) for word in review] for review in data]\n",
    "\n",
    "X_train = filter_out_of_vocab_indices(X_train, vocab_size)\n",
    "X_test = filter_out_of_vocab_indices(X_test, vocab_size)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Print the data\n",
    "print(X_train[0])\n",
    "print(y_train[0], \"-->\", class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Recurrent Neural Network\n",
    "Define and train a recurrent neural network using Keras, and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def answer_three():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len),  # Embedding layer, input_dim (vocab_size), output_dim (32), input_length (max_len)\n",
    "        keras.layers.SimpleRNN(64, dropout=0.3, activation='relu'),  # RNN layer, hidden size 64, dropout rate 0.3, ReLU\n",
    "        keras.layers.Dense(46, activation='softmax')  # Dense output layer, 46 neurons, softmax\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",  # Loss function\n",
    "        optimizer=\"adam\",  # Optimization algorithm: adam\n",
    "        metrics=[\"accuracy\"]  # Evaluation metrics: accuracy\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,  # Use X_train, y_train for training\n",
    "        epochs=10,  # epochs, 10\n",
    "        batch_size=32,  # batch size, 32\n",
    "        validation_data=(X_test, y_test)  # Use X_test, y_test for validation\n",
    "    )\n",
    "    \n",
    "    y_proba = model.predict(X_test)  # Calculate prediction probabilities on X_test\n",
    "    y_pred = np.argmax(y_proba, axis=1)  # Obtain the predicted classes from y_proba\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    microf1 = f1_score(y_test, y_pred, average='micro')  # Calculate micro f1\n",
    "    macrof1 = f1_score(y_test, y_pred, average='macro')  # Calculate macro f1\n",
    "    \n",
    "    return accuracy, microf1, macrof1\n",
    "\n",
    "# Run your function in the cell to return the results\n",
    "accuracy_3, microf1_3, macrof1_3 = answer_three()\n",
    "print(accuracy_3, microf1_3, macrof1_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
